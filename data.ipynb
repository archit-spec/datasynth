{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install duckdb\n",
        "%pip install --upgrade typing_extensions --quiet\n",
        "%pip install \"distilabel[openai,argilla]\" --quiet"
      ],
      "metadata": {
        "id": "45B6-JbOBx0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"distilabel[groq]\"\n",
        "!pip install instructor"
      ],
      "metadata": {
        "id": "im9QjNexhDlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "os.environ[\"HF_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "vKNI3hiAhCBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkoutdata(url):\n",
        "  import duckdb\n",
        "  con = duckdb.connect()\n",
        "  con.execute(\"INSTALL httpfs;\")\n",
        "  con.execute(\"LOAD httpfs;\")\n",
        "  df = con.sql(f\"SELECT * FROM '{url}'\").df()\n",
        "  return df"
      ],
      "metadata": {
        "id": "b5vKX_Frx0le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def"
      ],
      "metadata": {
        "id": "tJ8jA0bAhEtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.tasks.text_generation import EvolComplexityGeneratorTask\n",
        "task = EvolComplexityGeneratorTask()\n",
        "task.generate_prompt(\"Give three tips for staying healthy.\")\n",
        "Prompt(\n",
        "    system_prompt=\"\",\n",
        "    formatted_prompt=\"I want you to act as a Prompt ...\",\n",
        ")"
      ],
      "metadata": {
        "id": "4gQOTxzLNORM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.llms import OpenAILLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import LoadDataFromHub\n",
        "from distilabel.steps.tasks import TextGeneration\n",
        "from distilabel.llms import GroqLLM\n",
        "#from distilabel.steps.tasks import EvolComplexityTask, Prompt, EvolQualityTask, TextGenerationTask\n",
        "from distilabel.steps.tasks import TextGenerationTask\n",
        "\n",
        "with Pipeline(  #\n",
        "    name=\"simple-text-generation-pipeline\",\n",
        "    description=\"A simple text generation pipeline\",\n",
        ") as pipeline:  #\n",
        "    load_dataset = LoadDataFromHub(  #\n",
        "        name=\"load_dataset\",\n",
        "        split=\"test\",\n",
        "        num_examples=10,\n",
        "        output_mappings={\"question\": \"instruction\"},\n",
        "    )\n",
        "\n",
        "    text_generation = TextGeneration(  #\n",
        "        name=\"text_generation\",\n",
        "        llm = GroqLLM(\n",
        "            model=\"llama-3.2-3b-preview\",\n",
        "            task=EvolComplexityTask()\n",
        "            ),  #\n",
        "    )\n",
        "\n",
        "    load_dataset >> text_generation  #\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    distiset = pipeline.run(  #\n",
        "        parameters={\n",
        "            load_dataset.name: {\n",
        "                \"repo_id\": \"archit11/worldbuilding\",\n",
        "                \"split\": \"train\",\n",
        "            },\n",
        "            text_generation.name: {\n",
        "                \"llm\": {\n",
        "                    \"generation_kwargs\": {\n",
        "                        \"temperature\": 0.7,\n",
        "                        \"max_new_tokens\": 512,\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "    distiset.push_to_hub(repo_id=\"archit11/distilabel-example4\")  #"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dzj1fm5q88jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.llms import GroqLLM\n",
        "\n",
        "llm = GroqLLM(model=\"llama3-70b-8192\")\n",
        "\n",
        "llm.load()\n",
        "\n",
        "# Call the model\n",
        "output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from distilabel.llms import GroqLLM\n",
        "\n",
        "class User(BaseModel):\n",
        "    name: str\n",
        "    last_name: str\n",
        "    id: int\n",
        "\n",
        "llm = GroqLLM(\n",
        "    model=\"llama3-70b-8192\",\n",
        "    api_key=\"api.key\",\n",
        "    structured_output={\"schema\": User}\n",
        ")\n",
        "\n",
        "llm.load()\n",
        "\n",
        "output = llm.generate(inputs=[[{\"role\": \"user\", \"content\": \"Create a user profile for the following marathon\"}]])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EBzJiKRL51ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thCrnxJYBq3N"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "\n",
        "# Initialize DuckDB connection\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Install and load httpfs to read remote files\n",
        "con.execute(\"INSTALL httpfs;\")\n",
        "con.execute(\"LOAD httpfs;\")\n",
        "\n",
        "# URL of the Parquet file\n",
        "url = \"https://huggingface.co/datasets/openai/MMMLU/resolve/main/test/mmlu_HI-IN.csv?download=true\"\n",
        "\n",
        "# Query the Parquet file\n",
        "df = con.sql(f\"SELECT * FROM '{url}'\").df()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = con.sql(f\"SELECT * FROM '{url}'\").df()\n"
      ],
      "metadata": {
        "id": "Zgu3eTVMCBaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head(10)"
      ],
      "metadata": {
        "id": "vuYbo0FyERDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= df.tail(10)"
      ],
      "metadata": {
        "id": "oL3BbG_nTKgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a\n",
        "for i in a['answers']:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "nL1PO4rUUYzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "16eCRGJ_rh1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "from datasets import Dataset\n",
        "\n",
        "def create_hf_dataset(df, upvote_threshold=1):\n",
        "    dataset = []\n",
        "\n",
        "    for question, answers in zip(df['question'], df['answers']):\n",
        "        clean_question = BeautifulSoup(question, 'html.parser').get_text()\n",
        "\n",
        "        # Filter answers based on upvote threshold\n",
        "        valid_answers = [a for a in answers if a['pm_score'] >= upvote_threshold]\n",
        "\n",
        "        # Clean HTML tags from answers\n",
        "        for answer in valid_answers:\n",
        "            answer['text'] = BeautifulSoup(answer['text'], 'html.parser').get_text()\n",
        "\n",
        "        if valid_answers:\n",
        "            if len(valid_answers) > 1:\n",
        "                # Randomly club two answers\n",
        "                clubbed_answer = random.sample(valid_answers, 2)\n",
        "                dataset.append({\n",
        "                    \"question\": clean_question,\n",
        "                    \"answers\": [\n",
        "                        {\"author\": clubbed_answer[0]['author'], \"text\": clubbed_answer[0]['text']},\n",
        "                        {\"author\": clubbed_answer[1]['author'], \"text\": clubbed_answer[1]['text']}\n",
        "                    ]\n",
        "                })\n",
        "            else:\n",
        "                # Add a single answer if only one is valid\n",
        "                answer = valid_answers[0]\n",
        "                dataset.append({\n",
        "                    \"question\": clean_question,\n",
        "                    \"answers\": [\n",
        "                        {\"author\": answer['author'], \"text\": answer['text']}\n",
        "                    ]\n",
        "                })\n",
        "\n",
        "    return Dataset.from_list(dataset)\n",
        "\n",
        "# Usage\n",
        "# Assuming 'df' is your DataFrame\n",
        "hf_dataset = create_hf_dataset(df, upvote_threshold=1)\n",
        "\n",
        "# Save the dataset\n",
        "hf_dataset.save_to_disk(\"hello\")\n",
        "\n",
        "# Push to Hugging Face Hub (optional)\n",
        "# hf_dataset.push_to_hub(\"your-username/dataset-name\")"
      ],
      "metadata": {
        "id": "WW0stF_Oo3Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "\n",
        "def fetch_combine_parquet_files(base_url, num_files):\n",
        "    # Initialize DuckDB connection\n",
        "    con = duckdb.connect(':memory:')\n",
        "\n",
        "    # Install and load httpfs to read remote files\n",
        "    con.execute(\"INSTALL httpfs;\")\n",
        "    con.execute(\"LOAD httpfs;\")\n",
        "\n",
        "    # Create a list of file URLs\n",
        "    urls = [f\"{base_url}/train-{str(i).zfill(5)}-of-00004.parquet?download=true\" for i in range(num_files)]\n",
        "\n",
        "    # Construct the SQL query to union all files\n",
        "    query = \" UNION ALL \".join([f\"SELECT * FROM read_parquet('{url}')\" for url in urls])\n",
        "\n",
        "    # Execute the query and fetch the results as a DataFrame\n",
        "    df = con.execute(query).fetch_df()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Base URL of the Parquet files\n",
        "base_url = \"https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences/resolve/main/data/worldbuilding.stackexchange.com\"\n",
        "\n",
        "# Number of files to fetch (0 to 3, so 4 files total)\n",
        "num_files = 4\n",
        "\n",
        "# Fetch and combine the Parquet files\n",
        "df = fetch_combine_parquet_files(base_url, num_files)\n",
        "\n",
        "# Print some information about the combined DataFrame\n",
        "print(f\"Combined DataFrame shape: {df.shape}\")\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Optional: Save the combined DataFrame to a local Parquet file\n",
        "# df.to_parquet(\"combined_worldbuilding_data.parquet\")"
      ],
      "metadata": {
        "id": "WFoLQoTxsz6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from datasets import Dataset\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "def create_dpo_dataset(df, upvote_threshold=1):\n",
        "    dataset = []\n",
        "\n",
        "    for question, answers in zip(df['question'], df['answers']):\n",
        "        clean_question = BeautifulSoup(question, 'html.parser').get_text()\n",
        "\n",
        "        # Filter answers based on upvote threshold\n",
        "        valid_answers = [a for a in answers if a['pm_score'] >= upvote_threshold]\n",
        "\n",
        "        # Clean HTML tags from answers\n",
        "        for answer in valid_answers:\n",
        "            answer['text'] = BeautifulSoup(answer['text'], 'html.parser').get_text()\n",
        "\n",
        "        if len(valid_answers) >= 2:\n",
        "            # Sort answers by pm_score (upvotes) in descending order\n",
        "            sorted_answers = sorted(valid_answers, key=lambda x: x['pm_score'], reverse=True)\n",
        "\n",
        "            # Take the top answer as 'chosen' and the second as 'rejected'\n",
        "            chosen = sorted_answers[0]\n",
        "            rejected = sorted_answers[1]\n",
        "\n",
        "            dataset.append({\n",
        "                \"prompt\": clean_question,\n",
        "                \"chosen\": chosen['text'],\n",
        "                \"rejected\": rejected['text']\n",
        "            })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def save_as_parquet(dataset, file_path):\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(dataset)\n",
        "    table = pa.Table.from_pandas(df)\n",
        "    pq.write_table(table, file_path)\n",
        "\n",
        "def save_as_jsonl(dataset, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        for item in dataset:\n",
        "            json.dump(item, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "def upload_to_hub(dataset, repo_name, token):\n",
        "    # Create the repository if it doesn't exist\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        create_repo(repo_name, token=token)\n",
        "    except Exception as e:\n",
        "        print(f\"Repository already exists or couldn't be created: {e}\")\n",
        "\n",
        "    # Create Hugging Face Dataset\n",
        "    hf_dataset = Dataset.from_list(dataset)\n",
        "\n",
        "    # Push to Hugging Face Hub\n",
        "    hf_dataset.push_to_hub(repo_name, token=token)\n",
        "\n",
        "# Usage\n",
        "# Assuming 'df' is your DataFrame\n",
        "dpo_dataset = create_dpo_dataset(df, upvote_threshold=1)\n",
        "\n",
        "# Save as Parquet\n",
        "save_as_parquet(dpo_dataset, \"/content/worldbuilding_data_dpo/wb_dpo.parquet\")\n",
        "\n",
        "# Save as JSONL\n",
        "save_as_jsonl(dpo_dataset, \"/content/worldbuilding_data_dpo/wb_dpo.jsonl\")\n",
        "\n",
        "# Upload to Hugging Face Hub\n",
        "HF_TOKEN = \"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\"  # Replace with your actual token\n",
        "REPO_NAME = \"archit11/worldbuilding_dpo\"  # Replace with your desired repository name\n",
        "upload_to_hub(dpo_dataset, REPO_NAME, HF_TOKEN)\n",
        "\n",
        "print(f\"DPO Dataset uploaded successfully to {REPO_NAME}\")"
      ],
      "metadata": {
        "id": "Q26vQFL11KBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from datasets import Dataset\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "def create_qa_dataset(df, upvote_threshold=1):\n",
        "    dataset = []\n",
        "\n",
        "    for question, answers in zip(df['question'], df['answers']):\n",
        "        clean_question = BeautifulSoup(question, 'html.parser').get_text()\n",
        "\n",
        "        valid_answers = [a for a in answers if a['pm_score'] >= upvote_threshold]\n",
        "\n",
        "        for answer in valid_answers:\n",
        "            answer['text'] = BeautifulSoup(answer['text'], 'html.parser').get_text()\n",
        "\n",
        "        if valid_answers:\n",
        "            if len(valid_answers) > 1:\n",
        "                clubbed_answer = random.sample(valid_answers, 2)\n",
        "                dataset.append({\n",
        "                    \"question\": clean_question,\n",
        "                    \"answers\": [\n",
        "                        {\"author\": clubbed_answer[0]['author'], \"text\": clubbed_answer[0]['text']},\n",
        "                        {\"author\": clubbed_answer[1]['author'], \"text\": clubbed_answer[1]['text']}\n",
        "                    ]\n",
        "                })\n",
        "            else:\n",
        "                answer = valid_answers[0]\n",
        "                dataset.append({\n",
        "                    \"question\": clean_question,\n",
        "                    \"answers\": [\n",
        "                        {\"author\": answer['author'], \"text\": answer['text']}\n",
        "                    ]\n",
        "                })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def save_as_parquet(dataset, file_path):\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df['answers'] = df['answers'].apply(json.dumps)\n",
        "    table = pa.Table.from_pandas(df)\n",
        "    pq.write_table(table, file_path)\n",
        "\n",
        "def save_as_jsonl(dataset, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        for item in dataset:\n",
        "            json.dump(item, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "def upload_to_hub(dataset, repo_name, token):\n",
        "    # Create the repository if it doesn't exist\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        create_repo(repo_name, token=token)\n",
        "    except Exception as e:\n",
        "        print(f\"Repository already exists or couldn't be created: {e}\")\n",
        "\n",
        "    # Create Hugging Face Dataset\n",
        "    hf_dataset = Dataset.from_list(dataset)\n",
        "\n",
        "    # Push to Hugging Face Hub\n",
        "    hf_dataset.push_to_hub(repo_name, token=token)\n",
        "\n",
        "# Usage\n",
        "# Assuming 'df' is your DataFrame\n",
        "dataset = create_qa_dataset(df, upvote_threshold=1)\n",
        "\n",
        "# Save as Parquet\n",
        "save_as_parquet(dataset, \"/content/worldbuilding_data/wb.parquet\")\n",
        "\n",
        "# Save as JSONL\n",
        "save_as_jsonl(dataset, \"/content/worldbuilding_data/wb.jsonl\")\n",
        "\n",
        "# Upload to Hugging Face Hub\n",
        "HF_TOKEN = \"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\"  # Replace with your actual token\n",
        "REPO_NAME = \"archit11/worldbuilding\"  # Replace with your desired repository name\n",
        "upload_to_hub(dataset, REPO_NAME, HF_TOKEN)\n",
        "\n",
        "print(f\"Dataset uploaded successfully to {REPO_NAME}\")"
      ],
      "metadata": {
        "id": "OENstkFbqk_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPnEBhMzsRx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}